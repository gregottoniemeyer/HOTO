{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d2dfcf9-d7a5-48a1-9df8-f2e54a63797a",
   "metadata": {},
   "source": [
    "# Creating timeseries of the GOES data for 8 speakers\n",
    "\n",
    "- 8 channels (need 8 timeseries)\n",
    "- 12 minute piece, GOES samples every 5-minutes\n",
    "\n",
    "The artist wanted a viewer to feel like weather was moving through the room. \n",
    "\n",
    "This means we must think about time and space.\n",
    "\n",
    "Space:\n",
    "So, the 8 speakers should correspond to 8 different locations around the viewer. The spatial scale of the data variablity will need to be explored so we can determine the right distance these locations should be. Maybe if they are only 1 km apart, there isn't enough of a 'difference' for the sound to feel immersive.  But if the locations are too far apart, then they won't be correlated to eachother and may just sound like 8 unconnected sounds.\n",
    "\n",
    "Time: \n",
    "GOES CONUS data has 5-minute sampling. \n",
    "The piece is 12 minutes long - so if we went with realtime data, we would have 2 data points that could be interpolated together,\n",
    "but basically two gradient tones during 12 minutes is not very dynamic, \n",
    "but that might be okay? The other issues is that I think this surface data is meant to be dominate during only the first 3 minutes,\n",
    "so maybe we are only really looking at 3 minutes of gradients. If that is the case, in order to get a sense of weather, \n",
    "it might be necessary to create a compressed timeseries of data over the last hour or so.\n",
    "\n",
    "GOES:\n",
    "\n",
    "GOES has full disk, CONUS, mesoscale options as well as many different products.\n",
    "I'm using CONUS below as it has high spatial resolution, 5-minute sampling.\n",
    "\n",
    "## Steps in code below:\n",
    "- The code looks at two different GOES datasets. We don't really want one with cloud masks (missing data). \n",
    "- Georeference the coordinate system so we can look up data around a latitude and longitude\n",
    "- Calculate 8 points near a reference location (Miami)\n",
    "- Create timeseries of data at those points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ac0e35-0ac3-4544-aa9b-ab69969a458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from goes2go import GOES\n",
    "#import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "#import cartopy.crs as ccrs\n",
    "#import cartopy.feature as cfeature\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "#list of GOES products https://github.com/blaylockbk/goes2go/blob/main/goes2go/product_table.txt\n",
    "\n",
    "# Calculate latitude and longitude from GOES ABI fixed grid projection data\n",
    "#GOES ABI fixed grid projection is a map projection relative to the GOES satellite  \n",
    "#Units: latitude in 째N (째S < 0), longitude in 째E (째W < 0)  \n",
    "#See GOES-R Product User Guide (PUG) Volume 5 (L2 products) Section 4.2.8 for details & example of calculations  \n",
    "#\"file_id\" is an ABI L1b or L2 .nc file opened using the netCDF4 library  \n",
    "#code from https://www.star.nesdis.noaa.gov/atmospheric-composition-training/python_abi_lat_lon.php  \n",
    "#Acknowledgement:  NOAA/NESDIS/STAR Aerosols and Atmospheric Composition Science Team  \n",
    "#Their code is written for numpy arrays not xarray, so I updated it to work with xarray datasets  \n",
    "\n",
    "#import configuration location and filepath\n",
    "from myconfig import *\n",
    "output_path = output_path_data\n",
    "\n",
    "# Target latitude and longitude MIAMI\n",
    "#target_lat = 19.71361111111111  # Example: Latitude of Milwaukee Trench\n",
    "#target_lon = -67.31083333333333  # Example: Longitude of Milwaukee Trench\n",
    "\n",
    "#read the dataset so we know what we have already ready or not\n",
    "#gfile = xr.open_dataset(\"./data/goes_data_time.nc\").load()\n",
    "#gfile.close()\n",
    "\n",
    "def calculate_degrees(file_id):\n",
    "    # Read in GOES ABI fixed grid projection variables and constants\n",
    "    x_coordinate_1d = file_id.variables['x'][:]  # E/W scanning angle in radians\n",
    "    y_coordinate_1d = file_id.variables['y'][:]  # N/S elevation angle in radians\n",
    "    projection_info = file_id.variables['goes_imager_projection']\n",
    "    lon_origin = projection_info.attrs.get('longitude_of_projection_origin')\n",
    "    H = projection_info.attrs.get('perspective_point_height')+projection_info.attrs.get('semi_major_axis')\n",
    "    r_eq = projection_info.attrs.get('semi_major_axis')\n",
    "    r_pol = projection_info.attrs.get('semi_minor_axis')\n",
    "    \n",
    "    # Create 2D coordinate matrices from 1D coordinate vectors\n",
    "    x_coordinate_2d, y_coordinate_2d = np.meshgrid(x_coordinate_1d, y_coordinate_1d)\n",
    "    \n",
    "    # Equations to calculate latitude and longitude\n",
    "    lambda_0 = (lon_origin*np.pi)/180.0  \n",
    "    a_var = np.power(np.sin(x_coordinate_2d),2.0) + (np.power(np.cos(x_coordinate_2d),2.0)*(np.power(np.cos(y_coordinate_2d),2.0)+(((r_eq*r_eq)/(r_pol*r_pol))*np.power(np.sin(y_coordinate_2d),2.0))))\n",
    "    b_var = -2.0*H*np.cos(x_coordinate_2d)*np.cos(y_coordinate_2d)\n",
    "    c_var = (H**2.0)-(r_eq**2.0)\n",
    "    r_s = (-1.0*b_var - np.sqrt((b_var**2)-(4.0*a_var*c_var)))/(2.0*a_var)\n",
    "    s_x = r_s*np.cos(x_coordinate_2d)*np.cos(y_coordinate_2d)\n",
    "    s_y = - r_s*np.sin(x_coordinate_2d)\n",
    "    s_z = r_s*np.cos(x_coordinate_2d)*np.sin(y_coordinate_2d)\n",
    "    \n",
    "    # Ignore numpy errors for sqrt of negative number; occurs for GOES-16 ABI CONUS sector data\n",
    "    np.seterr(all='ignore')\n",
    "    \n",
    "    abi_lat = (180.0/np.pi)*(np.arctan(((r_eq*r_eq)/(r_pol*r_pol))*((s_z/np.sqrt(((H-s_x)*(H-s_x))+(s_y*s_y))))))\n",
    "    abi_lon = (lambda_0 - np.arctan(s_y/(H-s_x)))*(180.0/np.pi)\n",
    "    \n",
    "    return abi_lat, abi_lon\n",
    "\n",
    "def forward_fill_2d(arr):\n",
    "    # Loop through each column\n",
    "    for i in range(arr.shape[1]):\n",
    "        mask = np.isnan(arr[:, i])\n",
    "        # Forward fill NaN values\n",
    "        arr[mask, i] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), arr[~mask, i])\n",
    "    return arr\n",
    "\n",
    "def find_nearest_indices(lat_arr, lon_arr, target_lat, target_lon):\n",
    "    # Find the nearest latitude index\n",
    "    lat_idx = (np.abs(lat_arr - target_lat)).argmin()\n",
    "    # Find the nearest longitude index\n",
    "    lon_idx = (np.abs(lon_arr - target_lon)).argmin()\n",
    "    return lat_idx, lon_idx\n",
    "\n",
    "def calculate_points(istep,lon_idx,lat_idx):\n",
    "    #how big do we want to have the box?\n",
    "    #istep is how many grid points away from the center that we want to go   \n",
    "    # List of points you want to subset around point x\n",
    "    #   *  *  *\n",
    "    #   *  x  *\n",
    "    #   *  *  *\n",
    "    #north_point = [lat_idx+istep,lon_idx]\n",
    "    #east_point = [lat_idx,lon_idx+istep]\n",
    "    #south_point = [lat_idx-istep,lon_idx]\n",
    "    #west_point = [lat_idx,lon_idx-istep]\n",
    "    #northeast_point = [lat_idx+istep,lon_idx+istep]\n",
    "    #northwest_point = [lat_idx+istep,lon_idx-istep]\n",
    "    #southeast_point = [lat_idx-istep,lon_idx+istep]\n",
    "    #southwest_point = [lat_idx-istep,lon_idx-istep]\n",
    "    points = [\n",
    "        {\"i\": int(lon_idx), \"j\": int(lat_idx)+istep, \"name\": 'N'},\n",
    "        {\"i\": int(lon_idx)+istep, \"j\": int(lat_idx)+istep, \"name\": 'NE'},\n",
    "        {\"i\": int(lon_idx)+istep, \"j\": int(lat_idx), \"name\": 'East'},\n",
    "        {\"i\": int(lon_idx)+istep, \"j\": int(lat_idx)-istep, \"name\": 'SE'},\n",
    "        {\"i\": int(lon_idx), \"j\": int(lat_idx)-istep, \"name\": 'S'},\n",
    "        {\"i\": int(lon_idx)-istep, \"j\": int(lat_idx)-istep, \"name\": 'SW'},\n",
    "        {\"i\": int(lon_idx)-istep, \"j\": int(lat_idx), \"name\": 'W'},\n",
    "        {\"i\": int(lon_idx)-istep, \"j\": int(lat_idx)+istep, \"name\": 'NW'},\n",
    "    ]\n",
    "    return points\n",
    "\n",
    "def get_start_end_time(fname):\n",
    "    #goes filenames structure https://geonetcast.wordpress.com/2017/04/27/goes-16-file-naming-convention/\n",
    "    #use filename to find start/end times for data\n",
    "    tem = str(fname).split('/')\n",
    "    tem2,i = tem[5],25\n",
    "    dt_start = datetime.strptime(tem2[i:i+13], '%Y%j%H%M%S')\n",
    "    tem2,i = tem[5],41\n",
    "    dt_end = datetime.strptime(tem2[i:i+13], '%Y%j%H%M%S')\n",
    "    return dt_start,dt_end\n",
    "\n",
    "def already_read(fname,gfile):\n",
    "    start_time,end_time = get_start_end_time(fname)\n",
    "    isum = gfile.read_value.loc[start_time:end_time].sum().data\n",
    "    if isum>0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#def set_already_read(fname,gfile):\n",
    "#    start_time,end_time = get_start_end_time(fname)\n",
    "#    gfile.read_value.loc[start_time:end_time] = 1\n",
    "#    gfile.to_netcdf(\"./data/goes_data_time.nc\")\n",
    "#    return gfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75486994-e496-4ad6-9904-2581c223ad1a",
   "metadata": {},
   "source": [
    "# now we have some data points, we need to create a timeseries of data\n",
    "\n",
    "goes2go downloads the data before reading it  \n",
    "since we are looking at timeseries and there are like 288 files each day (5 min data)  \n",
    "i don't want to download all that data  \n",
    "so i'm trying to figure out if I can lazy load it  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de582dd-e9e9-4fbc-8022-c808eac69a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to S3 bucket with data\n",
    "#start_time='20231206'\n",
    "#end_time='20231213'\n",
    "#files_fname = './../../goes_filenames_test_'+start_time+'-'+end_time+'.csv'\n",
    "#df = pd.read_csv(files_fname)\n",
    "#df.file[0]\n",
    "fs = s3fs.S3FileSystem(anon=True) #connect to s3 bucket!\n",
    "#file_location=df.file\n",
    "#file_ob = [fs.open('s3://'+file) for file in file_location]        #open connection to files\n",
    "#print(file_ob[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca5c434-c54d-4f34-9e83-8b30a69c3e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c43e9-2541-41d1-a716-37173ab8710b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "<File-like object S3FileSystem, noaa-goes16/ABI-L2-MCMIPC/2020/341/00/OR_ABI-L2-MCMIPC-M6_G16_s20203410001098_e20203410003482_c20203410004020.nc>\n",
      "0 11\n",
      "0 12\n",
      "0 13\n",
      "0 14\n",
      "0 15\n",
      "0 16\n",
      "0 17\n",
      "0 18\n",
      "0 19\n",
      "0 20\n",
      "0 21\n",
      "0 22\n",
      "0 23\n",
      "0 24\n",
      "0 25\n",
      "0 26\n",
      "0 27\n",
      "0 28\n",
      "0 29\n",
      "0 30\n",
      "0 31\n",
      "0 32\n",
      "0 33\n",
      "0 34\n",
      "0 35\n",
      "0 36\n",
      "0 37\n",
      "0 38\n",
      "0 39\n",
      "0 40\n",
      "0 41\n",
      "0 42\n",
      "0 43\n",
      "0 44\n",
      "0 45\n",
      "0 46\n",
      "0 47\n",
      "0 48\n",
      "0 49\n",
      "0 50\n",
      "0 51\n",
      "0 52\n",
      "0 53\n",
      "0 54\n",
      "0 55\n",
      "0 56\n",
      "0 57\n",
      "0 58\n",
      "0 59\n",
      "0 60\n",
      "0 61\n",
      "0 62\n",
      "0 63\n",
      "0 64\n",
      "0 65\n",
      "0 66\n",
      "0 67\n",
      "0 68\n",
      "0 69\n",
      "0 70\n",
      "0 71\n",
      "0 72\n",
      "0 73\n",
      "0 74\n",
      "0 75\n",
      "0 76\n",
      "0 77\n",
      "0 78\n",
      "0 79\n",
      "0 80\n",
      "0 81\n",
      "0 82\n",
      "0 83\n",
      "0 84\n",
      "0 85\n",
      "0 86\n"
     ]
    }
   ],
   "source": [
    "for incr in range(0,number_days_to_process):\n",
    "\n",
    "    date_start = datetime(start_year, start_month, start_day) + timedelta(days=incr)\n",
    "    date_end = date_start + timedelta(days=1)  \n",
    "\n",
    "    start_time = date_start.strftime(\"%Y%m%d\")\n",
    "    end_time = date_end.strftime(\"%Y%m%d\")\n",
    "    files_fname = output_path_fname+'goes_filenames_test_'+start_time+'-'+end_time+'.csv'\n",
    "    df = pd.read_csv(files_fname)\n",
    "\n",
    "    nc_fname = output_path_data+'goes_timeseries_'+target_name+'_time_'+start_time+'-'+end_time+'.nc'\n",
    "    i_new=0\n",
    "    if os.path.exists(nc_fname):\n",
    "        all_data=xr.open_dataset(nc_fname)\n",
    "        all_data.close()\n",
    "        i_new = len(all_data.time)+1\n",
    "    print(i_new)\n",
    "    \n",
    "    file_location=df.file\n",
    "    file_ob = [fs.open('s3://'+file) for file in file_location]        #open connection to files\n",
    "    print(file_ob[0])\n",
    "\n",
    "#    if incr==3:\n",
    "#        iii=209\n",
    "#        ds = xr.open_dataset(\".\\..\\goes_timeseries_20231209-20231210.nc\")\n",
    "#        ds.load()\n",
    "#        ds.close()\n",
    "#    else:\n",
    "#        iii=1\n",
    "    \n",
    "    for i in range(i_new,len(file_ob)):\n",
    "        fname = file_ob[i]\n",
    "    #    if already_read(fname,gfile):\n",
    "    #        continue    \n",
    "        ds = xr.open_dataset(file_ob[i]) #note file is super messed up formatting\n",
    "        #ds2 = ds#.isel(time=0)\n",
    "        #calculate lat/lon\n",
    "        abi_lat, abi_lon = calculate_degrees(ds)\n",
    "        abi_lat = forward_fill_2d(abi_lat.copy())\n",
    "        abi_lon = forward_fill_2d(abi_lon.copy())\n",
    "        #note this isn't going to be perfect because used 1d so run one time, then again with closer values\n",
    "        # Find nearest indices\n",
    "        lat_idx, lon_idx = find_nearest_indices(abi_lat[:,0], abi_lon[0,:], target_lat, target_lon)\n",
    "        lat_idx, lon_idx = find_nearest_indices(abi_lat[:,lon_idx], abi_lon[lat_idx,:], target_lat, target_lon)\n",
    "        stime,etime = get_start_end_time(fname)\n",
    "        ds_tem = ds.isel(y=slice(lat_idx-25,lat_idx+25),x=slice(lon_idx-25,lon_idx+25)).CMI_C10.load()\n",
    "        for istep in range(1,20):\n",
    "            points = calculate_points(istep,25,25) #lon_idx,lat_idx)\n",
    "            point_data=ds_tem.isel(y=points[0].get('j'), x=points[0].get('i')) #, method=\"nearest\")\n",
    "            for p in range(len(points)):\n",
    "                if p>0:\n",
    "                    tem=ds_tem.isel(y=points[p].get('j'), x=points[p].get('i')) #, method=\"nearest\")\n",
    "                    point_data = xr.concat([point_data, tem], dim=\"points_index\")\n",
    "            if istep>1:\n",
    "                step_data = xr.concat([step_data,point_data], dim=\"step\")\n",
    "            else:\n",
    "                step_data = point_data\n",
    "        if i>1:\n",
    "            step_data=step_data.to_dataset()\n",
    "            all_data = xr.concat([all_data,step_data], dim=\"time\")\n",
    "        else:\n",
    "            all_data = step_data\n",
    "        nc_fname = output_path_data+'goes_timeseries_'+target_name+'_time_'+start_time+'-'+end_time+'.nc'\n",
    "        csv_fname = output_path_data+'goes_timeseries_'+target_name+'_time_'+start_time+'-'+end_time+'.csv'\n",
    "        all_data.to_netcdf(nc_fname)\n",
    "    \n",
    "        all_data_df = all_data.to_dataframe()\n",
    "        all_data_df.to_csv(csv_fname)\n",
    "        \n",
    "        print(incr,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65d909-f64f-4412-bfd7-0ab79df4ba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_data.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aea959-1cec-4ba1-87bc-aa281a1d4d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "    nc_fname = output_path_data+'goes_timeseries_'+target_name+'_time_'+start_time+'-'+end_time+'.nc'\n",
    "    i_new=0\n",
    "    if os.path.exists(nc_fname):\n",
    "        all_data=xr.open_dataset(nc_fname)\n",
    "        all_data.close()\n",
    "        i_new = len(all_data.time)\n",
    "    print(i_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d013013-bdf5-4626-829e-c87712484429",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.t.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a56b73-5b3b-40db-ab15-c2b85760cdf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4e2da5-cc0b-4568-98ca-8faca1b64178",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_data=step_data.to_dataset()\n",
    "if i>1:\n",
    "    all_data = xr.concat([all_data,step_data], dim=\"time\")\n",
    "else:\n",
    "    all_data = step_data\n",
    "nc_fname = output_path_data+'goes_timeseries_'+target_name+'_time_'+start_time+'-'+end_time+'.nc'\n",
    "csv_fname = output_path_data+'goes_timeseries_'+target_name+'_time_'+start_time+'-'+end_time+'.csv'\n",
    "all_data.to_netcdf(nc_fname)\n",
    "    \n",
    "all_data_df = all_data.to_dataframe()\n",
    "all_data_df.to_csv(csv_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be137a1d-a785-4be2-9673-d9ff4dc12ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_data.t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd44bb-caca-44bc-8c46-9b1d4e5f6074",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.t[0:10].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbae7d6-94c2-419f-af89-c6b86f637652",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.isel(time=slice(0,10))\n",
    "all_data.t[0:10].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a333c5-3252-47d4-afdf-9e2d76914f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_fname = output_path_data+'goes_timeseries_'+target_name+'_time_'+start_time+'-'+end_time+'.nc'\n",
    "csv_fname = output_path_data+'goes_timeseries_'+target_name+'_time_'+start_time+'-'+end_time+'.csv'\n",
    "all_data.to_netcdf(nc_fname)\n",
    "    \n",
    "all_data_df = all_data.to_dataframe()\n",
    "all_data_df.to_csv(csv_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d646a2ad-01b5-4789-bf51-d9b041c68b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.open_dataset('C:\\\\Users\\\\clwhit13\\\\Desktop\\\\github\\\\HOTO\\\\data\\\\MilwaukeeTrench_data\\\\goes_timeseries_MilwaukeeTrench_time_20201206-20201207.nc') as ds:\n",
    "    # Perform operations with the dataset\n",
    "    print(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a2e028-5623-4bb0-a4c1-8f1f4a794fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee5a32d-64b5-476c-a67b-b626f2c4b127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
