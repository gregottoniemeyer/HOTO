{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d2dfcf9-d7a5-48a1-9df8-f2e54a63797a",
   "metadata": {},
   "source": [
    "# Creating timeseries of the GOES data for 8 speakers\n",
    "\n",
    "- 8 channels (need 8 timeseries)\n",
    "- 12 minute piece, GOES samples every 5-minutes\n",
    "\n",
    "The artist wanted a viewer to feel like weather was moving through the room. \n",
    "\n",
    "This means we must think about time and space.\n",
    "\n",
    "Space:\n",
    "So, the 8 speakers should correspond to 8 different locations around the viewer. The spatial scale of the data variablity will need to be explored so we can determine the right distance these locations should be. Maybe if they are only 1 km apart, there isn't enough of a 'difference' for the sound to feel immersive.  But if the locations are too far apart, then they won't be correlated to eachother and may just sound like 8 unconnected sounds.\n",
    "\n",
    "Time: \n",
    "GOES CONUS data has 5-minute sampling. \n",
    "The piece is 12 minutes long - so if we went with realtime data, we would have 2 data points that could be interpolated together,\n",
    "but basically two gradient tones during 12 minutes is not very dynamic, \n",
    "but that might be okay? The other issues is that I think this surface data is meant to be dominate during only the first 3 minutes,\n",
    "so maybe we are only really looking at 3 minutes of gradients. If that is the case, in order to get a sense of weather, \n",
    "it might be necessary to create a compressed timeseries of data over the last hour or so.\n",
    "\n",
    "GOES:\n",
    "\n",
    "GOES has full disk, CONUS, mesoscale options as well as many different products.\n",
    "I'm using CONUS below as it has high spatial resolution, 5-minute sampling.\n",
    "\n",
    "## Steps in code below:\n",
    "- The code looks at two different GOES datasets. We don't really want one with cloud masks (missing data). \n",
    "- Georeference the coordinate system so we can look up data around a latitude and longitude\n",
    "- Calculate 8 points near a reference location (Miami)\n",
    "- Create timeseries of data at those points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ac0e35-0ac3-4544-aa9b-ab69969a458f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T23:59:15.157043Z",
     "iopub.status.busy": "2024-10-25T23:59:15.156509Z",
     "iopub.status.idle": "2024-10-25T23:59:15.911915Z",
     "shell.execute_reply": "2024-10-25T23:59:15.911161Z"
    }
   },
   "outputs": [],
   "source": [
    "#from goes2go import GOES\n",
    "#import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "#import cartopy.crs as ccrs\n",
    "#import cartopy.feature as cfeature\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "#import gc\n",
    "#gc.set_debug(gc.DEBUG_LEAK)\n",
    "\n",
    "#list of GOES products https://github.com/blaylockbk/goes2go/blob/main/goes2go/product_table.txt\n",
    "\n",
    "# Calculate latitude and longitude from GOES ABI fixed grid projection data\n",
    "#GOES ABI fixed grid projection is a map projection relative to the GOES satellite  \n",
    "#Units: latitude in 째N (째S < 0), longitude in 째E (째W < 0)  \n",
    "#See GOES-R Product User Guide (PUG) Volume 5 (L2 products) Section 4.2.8 for details & example of calculations  \n",
    "#\"file_id\" is an ABI L1b or L2 .nc file opened using the netCDF4 library  \n",
    "#code from https://www.star.nesdis.noaa.gov/atmospheric-composition-training/python_abi_lat_lon.php  \n",
    "#Acknowledgement:  NOAA/NESDIS/STAR Aerosols and Atmospheric Composition Science Team  \n",
    "#Their code is written for numpy arrays not xarray, so I updated it to work with xarray datasets  \n",
    "\n",
    "#import configuration location and filepath\n",
    "from myconfig import *\n",
    "\n",
    "output_path_data = output_path_goes_data\n",
    "output_path_fname = output_path_goes_fname\n",
    "\n",
    "def calculate_degrees(file_id):\n",
    "    # Read in GOES ABI fixed grid projection variables and constants\n",
    "    x_coordinate_1d = file_id.variables['x'][:]  # E/W scanning angle in radians\n",
    "    y_coordinate_1d = file_id.variables['y'][:]  # N/S elevation angle in radians\n",
    "    projection_info = file_id.variables['goes_imager_projection']\n",
    "    lon_origin = projection_info.attrs.get('longitude_of_projection_origin')\n",
    "    H = projection_info.attrs.get('perspective_point_height')+projection_info.attrs.get('semi_major_axis')\n",
    "    r_eq = projection_info.attrs.get('semi_major_axis')\n",
    "    r_pol = projection_info.attrs.get('semi_minor_axis')\n",
    "    \n",
    "    # Create 2D coordinate matrices from 1D coordinate vectors\n",
    "    x_coordinate_2d, y_coordinate_2d = np.meshgrid(x_coordinate_1d, y_coordinate_1d)\n",
    "    \n",
    "    # Equations to calculate latitude and longitude\n",
    "    lambda_0 = (lon_origin*np.pi)/180.0  \n",
    "    a_var = np.power(np.sin(x_coordinate_2d),2.0) + (np.power(np.cos(x_coordinate_2d),2.0)*(np.power(np.cos(y_coordinate_2d),2.0)+(((r_eq*r_eq)/(r_pol*r_pol))*np.power(np.sin(y_coordinate_2d),2.0))))\n",
    "    b_var = -2.0*H*np.cos(x_coordinate_2d)*np.cos(y_coordinate_2d)\n",
    "    c_var = (H**2.0)-(r_eq**2.0)\n",
    "    r_s = (-1.0*b_var - np.sqrt((b_var**2)-(4.0*a_var*c_var)))/(2.0*a_var)\n",
    "    s_x = r_s*np.cos(x_coordinate_2d)*np.cos(y_coordinate_2d)\n",
    "    s_y = - r_s*np.sin(x_coordinate_2d)\n",
    "    s_z = r_s*np.cos(x_coordinate_2d)*np.sin(y_coordinate_2d)\n",
    "    \n",
    "    # Ignore numpy errors for sqrt of negative number; occurs for GOES-16 ABI CONUS sector data\n",
    "    np.seterr(all='ignore')\n",
    "    \n",
    "    abi_lat = (180.0/np.pi)*(np.arctan(((r_eq*r_eq)/(r_pol*r_pol))*((s_z/np.sqrt(((H-s_x)*(H-s_x))+(s_y*s_y))))))\n",
    "    abi_lon = (lambda_0 - np.arctan(s_y/(H-s_x)))*(180.0/np.pi)\n",
    "    \n",
    "    return abi_lat, abi_lon\n",
    "\n",
    "def forward_fill_2d(arr):\n",
    "    # Loop through each column\n",
    "    for i in range(arr.shape[1]):\n",
    "        mask = np.isnan(arr[:, i])\n",
    "        # Forward fill NaN values\n",
    "        arr[mask, i] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), arr[~mask, i])\n",
    "    return arr\n",
    "\n",
    "def find_nearest_indices(lat_arr, lon_arr, target_lat, target_lon):\n",
    "    # Find the nearest latitude index\n",
    "    lat_idx = (np.abs(lat_arr - target_lat)).argmin()\n",
    "    # Find the nearest longitude index\n",
    "    lon_idx = (np.abs(lon_arr - target_lon)).argmin()\n",
    "    return lat_idx, lon_idx\n",
    "\n",
    "def calculate_points(istep,lon_idx,lat_idx):\n",
    "    #how big do we want to have the box?\n",
    "    #istep is how many grid points away from the center that we want to go   \n",
    "    # List of points you want to subset around point x\n",
    "    #   *  *  *\n",
    "    #   *  x  *\n",
    "    #   *  *  *\n",
    "    #north_point = [lat_idx+istep,lon_idx]\n",
    "    #east_point = [lat_idx,lon_idx+istep]\n",
    "    #south_point = [lat_idx-istep,lon_idx]\n",
    "    #west_point = [lat_idx,lon_idx-istep]\n",
    "    #northeast_point = [lat_idx+istep,lon_idx+istep]\n",
    "    #northwest_point = [lat_idx+istep,lon_idx-istep]\n",
    "    #southeast_point = [lat_idx-istep,lon_idx+istep]\n",
    "    #southwest_point = [lat_idx-istep,lon_idx-istep]\n",
    "    points = [\n",
    "        {\"i\": int(lon_idx), \"j\": int(lat_idx)+istep, \"name\": 'N'},\n",
    "        {\"i\": int(lon_idx)+istep, \"j\": int(lat_idx)+istep, \"name\": 'NE'},\n",
    "        {\"i\": int(lon_idx)+istep, \"j\": int(lat_idx), \"name\": 'East'},\n",
    "        {\"i\": int(lon_idx)+istep, \"j\": int(lat_idx)-istep, \"name\": 'SE'},\n",
    "        {\"i\": int(lon_idx), \"j\": int(lat_idx)-istep, \"name\": 'S'},\n",
    "        {\"i\": int(lon_idx)-istep, \"j\": int(lat_idx)-istep, \"name\": 'SW'},\n",
    "        {\"i\": int(lon_idx)-istep, \"j\": int(lat_idx), \"name\": 'W'},\n",
    "        {\"i\": int(lon_idx)-istep, \"j\": int(lat_idx)+istep, \"name\": 'NW'},\n",
    "    ]\n",
    "    return points\n",
    "\n",
    "def get_start_end_time(fname):\n",
    "    #goes filenames structure https://geonetcast.wordpress.com/2017/04/27/goes-16-file-naming-convention/\n",
    "    #use filename to find start/end times for data\n",
    "    tem = str(fname).split('/')\n",
    "    tem2,i = tem[5],25\n",
    "    dt_start = datetime.strptime(tem2[i:i+13], '%Y%j%H%M%S')\n",
    "    tem2,i = tem[5],41\n",
    "    dt_end = datetime.strptime(tem2[i:i+13], '%Y%j%H%M%S')\n",
    "    return dt_start,dt_end\n",
    "\n",
    "def already_read(fname,gfile):\n",
    "    start_time,end_time = get_start_end_time(fname)\n",
    "    isum = gfile.read_value.loc[start_time:end_time].sum().data\n",
    "    if isum>0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=True) #connect to s3 bucket!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75486994-e496-4ad6-9904-2581c223ad1a",
   "metadata": {},
   "source": [
    "# now we have some data points, we need to create a timeseries of data\n",
    "\n",
    "goes2go downloads the data before reading it  \n",
    "since we are looking at timeseries and there are like 288 files each day (5 min data)  \n",
    "i don't want to download all that data  \n",
    "so i'm trying to figure out if I can lazy load it  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85dbc3e6-a2e0-4663-bb9a-fc59b9f23a76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T23:59:15.914767Z",
     "iopub.status.busy": "2024-10-25T23:59:15.914204Z",
     "iopub.status.idle": "2024-10-25T23:59:15.919841Z",
     "shell.execute_reply": "2024-10-25T23:59:15.919174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024 10 25 23\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "start_year = int(now.strftime(\"%Y\"))\n",
    "start_month = int(now.strftime(\"%m\"))\n",
    "start_day = int(now.strftime(\"%d\"))\n",
    "start_hour = int(now.strftime(\"%H\"))\n",
    "print(start_year, start_month, start_day,start_hour)\n",
    "\n",
    "#print(start_year, start_month, start_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "635c43e9-2541-41d1-a716-37173ab8710b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T23:59:15.922315Z",
     "iopub.status.busy": "2024-10-25T23:59:15.922037Z",
     "iopub.status.idle": "2024-10-26T00:00:25.480376Z",
     "shell.execute_reply": "2024-10-26T00:00:25.479685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20241017 20241018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<File-like object S3FileSystem, noaa-goes16/ABI-L2-MCMIPC/2024/291/00/OR_ABI-L2-MCMIPC-M6_G16_s20242910001171_e20242910003556_c20242910004066.nc>\n",
      "20241018 20241019\n",
      "288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<File-like object S3FileSystem, noaa-goes16/ABI-L2-MCMIPC/2024/292/00/OR_ABI-L2-MCMIPC-M6_G16_s20242920001171_e20242920003550_c20242920004066.nc>\n",
      "20241019 20241020\n",
      "288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<File-like object S3FileSystem, noaa-goes16/ABI-L2-MCMIPC/2024/293/00/OR_ABI-L2-MCMIPC-M6_G16_s20242930001172_e20242930003545_c20242930004068.nc>\n",
      "20241020 20241021\n",
      "288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<File-like object S3FileSystem, noaa-goes16/ABI-L2-MCMIPC/2024/294/00/OR_ABI-L2-MCMIPC-M6_G16_s20242940001172_e20242940003551_c20242940004074.nc>\n",
      "20241021 20241022\n",
      "288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<File-like object S3FileSystem, noaa-goes16/ABI-L2-MCMIPC/2024/295/00/OR_ABI-L2-MCMIPC-M6_G16_s20242950001173_e20242950003559_c20242950004062.nc>\n",
      "20241022 20241023\n",
      "288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<File-like object S3FileSystem, noaa-goes16/ABI-L2-MCMIPC/2024/296/00/OR_ABI-L2-MCMIPC-M6_G16_s20242960001173_e20242960003559_c20242960004069.nc>\n",
      "20241023 20241024\n",
      "287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<File-like object S3FileSystem, noaa-goes16/ABI-L2-MCMIPC/2024/297/00/OR_ABI-L2-MCMIPC-M6_G16_s20242970001174_e20242970003547_c20242970004059.nc>\n",
      "20241024 20241025\n",
      "288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<File-like object S3FileSystem, noaa-goes16/ABI-L2-MCMIPC/2024/298/00/OR_ABI-L2-MCMIPC-M6_G16_s20242980001174_e20242980003547_c20242980004062.nc>\n",
      "20241025 20241026\n",
      "287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<File-like object S3FileSystem, noaa-goes16/ABI-L2-MCMIPC/2024/299/00/OR_ABI-L2-MCMIPC-M6_G16_s20242990001164_e20242990003548_c20242990004070.nc>\n"
     ]
    }
   ],
   "source": [
    "for incr in range(number_days_to_process,-1,-1):\n",
    "\n",
    "    date_start = datetime(start_year, start_month, start_day) - timedelta(days=incr)\n",
    "    start_time = date_start.strftime(\"%Y%m%d\")\n",
    "\n",
    "    # if you try to search for files in advance of when they are created the program dies\n",
    "    # so if it is today, the program only searches (date_end) for files up to now)\n",
    "    if incr == 0:\n",
    "        date_end = now #date_start + timedelta(days=1)  \n",
    "        end_time = (date_start + timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "    else:\n",
    "        date_end = date_start + timedelta(days=1)  \n",
    "        end_time = date_end.strftime(\"%Y%m%d\")\n",
    "\n",
    "    print(start_time,end_time)\n",
    "    files_fname = output_path_fname+'goes_filenames_test_'+start_time+'-'+end_time+'.csv'\n",
    "    df = pd.read_csv(files_fname)\n",
    "\n",
    "    nc_fname = output_path_data+'goes_timeseries_'+target_name+'_time_'+start_time+'-'+end_time+'.nc'\n",
    "    i_new=0\n",
    "    if os.path.exists(nc_fname):\n",
    "        all_data=xr.open_dataset(nc_fname)\n",
    "        all_data.close()\n",
    "        i_new = len(all_data.time)+1\n",
    "    print(i_new)\n",
    "    \n",
    "    file_location=df.file\n",
    "    file_ob = [fs.open('s3://'+file) for file in file_location]        #open connection to files\n",
    "    print(file_ob[0])\n",
    "   \n",
    "    for i in range(i_new,len(file_ob)):\n",
    "        fname = file_ob[i]\n",
    "        ds = xr.open_dataset(file_ob[i]) #note file is super messed up formatting\n",
    "        #calculate lat/lon\n",
    "        abi_lat, abi_lon = calculate_degrees(ds)\n",
    "        abi_lat = forward_fill_2d(abi_lat.copy())\n",
    "        abi_lon = forward_fill_2d(abi_lon.copy())\n",
    "        #note this isn't going to be perfect because used 1d so run one time, then again with closer values\n",
    "        # Find nearest indices\n",
    "        lat_idx, lon_idx = find_nearest_indices(abi_lat[:,0], abi_lon[0,:], target_lat, target_lon)\n",
    "        lat_idx, lon_idx = find_nearest_indices(abi_lat[:,lon_idx], abi_lon[lat_idx,:], target_lat, target_lon)\n",
    "        stime,etime = get_start_end_time(fname)\n",
    "        ds_tem = ds.isel(y=slice(lat_idx-25,lat_idx+25),x=slice(lon_idx-25,lon_idx+25)).CMI_C10.load()\n",
    "        ds.close()\n",
    "        for istep in range(1,20):\n",
    "            points = calculate_points(istep,25,25) #lon_idx,lat_idx)\n",
    "            point_data=ds_tem.isel(y=points[0].get('j'), x=points[0].get('i')) #, method=\"nearest\")\n",
    "            for p in range(len(points)):\n",
    "                if p>0:\n",
    "                    tem=ds_tem.isel(y=points[p].get('j'), x=points[p].get('i')) # method=\"nearest\")\n",
    "                    point_data = xr.concat([point_data, tem], dim=\"points_index\")\n",
    "            if istep>1:\n",
    "                step_data = xr.concat([step_data,point_data], dim=\"step\")\n",
    "            else:\n",
    "                step_data = point_data\n",
    "        step_data=step_data.to_dataset()\n",
    "        if i>1:\n",
    "            all_data = xr.concat([all_data,step_data], dim=\"time\")\n",
    "        else:\n",
    "            all_data = step_data\n",
    "        nc_fname = output_path_data+'goes_timeseries_'+target_name+'_time_'+start_time+'-'+end_time+'.nc'\n",
    "        csv_fname = output_path_data+'goes_timeseries_'+target_name+'_time_'+start_time+'-'+end_time+'.csv'\n",
    "        all_data.to_netcdf(nc_fname)\n",
    "    \n",
    "        all_data_df = all_data.to_dataframe()\n",
    "        all_data_df.to_csv(csv_fname)\n",
    "        del ds_tem\n",
    "        print(incr,i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "user-env:(heart)",
   "language": "python",
   "name": "heart"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
