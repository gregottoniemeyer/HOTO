{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc29b9a3-06b6-4016-be2c-015ce9cfa3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "#import configuration location and filepath\n",
    "from myconfig import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c0b084-792f-4f83-8103-def24e7fdd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get csv files\n",
    "#out: merged_df.to_csv(\"../data/4min_pass_wave_salt_temp.csv\", index=True)\n",
    "\n",
    "data_location_01 = output_path_ecco + \"trench_dive_temperature.csv\"\n",
    "data_location_02 = output_path_ecco + \"trench_dive_salinity.csv\"\n",
    "data_location_03 = output_path_ecco + \"trench_dive_current_U.csv\"\n",
    "data_location_04 = output_path_ecco + \"trench_dive_current_V.csv\"\n",
    "data_location_05 = output_path_ecco + \"trench_dive_current_W.csv\"\n",
    "data_location_06 = output_path_nrt + \"buoy_nrt.csv\"\n",
    "data_location_07 = output_path_nrt + \"seismic_7200.csv\"\n",
    "data_location_08 = output_path_dolphin + \"dolphins_01.csv\"\n",
    "data_location_09 = output_path_nrt + \"nrt_raining.csv\"\n",
    "data_location_10 = output_path_nrt + \"goes_timeseries_nrt.csv\"\n",
    "data_location_11 = output_path_dolphin + \"dolphins.csv\"\n",
    "\n",
    "df_temp = pd.read_csv(data_location_01)\n",
    "df_salinity = pd.read_csv(data_location_02)\n",
    "df_U = pd.read_csv(data_location_03)\n",
    "df_V = pd.read_csv(data_location_04)\n",
    "df_W = pd.read_csv(data_location_05)\n",
    "df_buoy = pd.read_csv(data_location_06)\n",
    "df_seismic = pd.read_csv(data_location_07)\n",
    "df_creatures = pd.read_csv(data_location_08)\n",
    "df_rain = pd.read_csv(data_location_09)\n",
    "df_clouds = pd.read_csv(data_location_10)\n",
    "df_dolphins = pd.read_csv(data_location_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98520a66-cd27-44bf-88b9-15bf05a7b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for min-max normalization per column\n",
    "def min_max_normalize_column(df):\n",
    "    return df.apply(lambda col: (col - col.min()) / (col.max() - col.min()), axis=0)\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "df_temp = pd.read_csv(data_location_01)\n",
    "df_salinity = pd.read_csv(data_location_02)\n",
    "df_U = pd.read_csv(data_location_03)\n",
    "df_V = pd.read_csv(data_location_04)\n",
    "df_W = pd.read_csv(data_location_05)\n",
    "\n",
    "#get pressure\n",
    "# Constants\n",
    "rho = 1025  # kg/m³, density of seawater\n",
    "g = 9.81  # m/s², gravitational acceleration\n",
    "P0 = 101325  # Pa, atmospheric pressure at sea level\n",
    "\n",
    "# Calculate pressure in Pascals using the absolute value of the depth\n",
    "df_temp['Pressure'] = P0 + rho * g * abs(df_temp['depth'])\n",
    "\n",
    "# List of the original DataFrames\n",
    "dfs = [df_temp, df_salinity, df_U, df_V, df_W]\n",
    "\n",
    "# Process each DataFrame\n",
    "new_dfs = []\n",
    "for df in dfs:\n",
    "    # Normalize each column in the DataFrame using min-max normalization\n",
    "    normalized_df = min_max_normalize_column(df)\n",
    "    \n",
    "    # Get the first row of the original DataFrame\n",
    "    first_values = normalized_df.iloc[0]\n",
    "    \n",
    "    # Create a new DataFrame with 7200 rows\n",
    "    # Fill the first 1800 rows with the first value of each respective column\n",
    "    new_df = pd.DataFrame(index=range(7200), columns=df.columns)\n",
    "\n",
    "    # Assign the first 1800 rows with the first values of the original DataFrame columns\n",
    "    new_df.iloc[:1800] = first_values\n",
    "\n",
    "    # Insert the normalized data starting from index 1800\n",
    "    new_df.iloc[1800:1800 + len(normalized_df), :] = normalized_df.values\n",
    "\n",
    "    # Eliminate NaN (forward fill)\n",
    "    new_df = new_df.ffill()\n",
    "    \n",
    "    # Append the new DataFrame to the list\n",
    "    new_dfs.append(new_df)\n",
    "\n",
    "# Unpack the new DataFrames into respective variables\n",
    "new_df_temp, new_df_salinity, new_df_U, new_df_V, new_df_W = new_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e7be84-a47d-4b27-bd4d-69e58cf98fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temp['Pressure'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915e3b8d-356d-4eec-b188-1f0f95041732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df_W.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0982bfb-e664-494a-bcd3-654147d786d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "# Define the number of unique colors equal to the number of columns\n",
    "colors = cm.get_cmap('tab20', len(df_temp.columns))  # Using 'tab10' colormap (or any other colormap)\n",
    "\n",
    "# Create a single plot for all columns\n",
    "fig, ax1 = plt.subplots(figsize=(16, 4))\n",
    "\n",
    "# Loop through each column and plot on the same graph\n",
    "for idx, column in enumerate(df_temp.columns):\n",
    "    ax1.plot(df_temp.index, df_temp[column], color=colors(idx), label=column)\n",
    "\n",
    "# Set labels for the x and y axes\n",
    "ax1.set_xlabel('Index')\n",
    "ax1.set_ylabel('Values')\n",
    "\n",
    "# Add a combined legend for all columns and place it below the plot\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=len(normalized_df.columns))\n",
    "\n",
    "# Set a title for the plot\n",
    "plt.title('All Channels on the Same Graph')\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "#plt.savefig(output_path_figures+\"dive.png\")\n",
    "\n",
    "# Display the plot\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c357e-b123-47ac-8fd9-5e00f80deb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df_salinity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4038c65d-db4a-424f-8ccf-4e8fd8933265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_buoy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a77ea6-0a78-4196-87fb-b8d15e7f8ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dolphin sidequest\n",
    "df_dolphins = df_dolphins[[\"Lizzy_angle\",\"Perry_angle\",\"BentHighNick_angle\"]]\n",
    "df_dolphins = df_dolphins.astype(float)\n",
    "df_dolphins.fillna(0.0, inplace=True)\n",
    "df_dolphins_normalized = min_max_normalize_column(df_dolphins)\n",
    "\n",
    "# Remove rows where all three specified columns are 0.0\n",
    "filtered_df_dolphins = df_dolphins_normalized[~((df_dolphins_normalized[\"Lizzy_angle\"] == 0.0) & (df_dolphins_normalized[\"Perry_angle\"] == 0.0) & (df_dolphins_normalized[\"BentHighNick_angle\"] == 0.0))]\n",
    "\n",
    "# Reset the index to reindex the DataFrame\n",
    "filtered_df_dolphins = filtered_df_dolphins.reset_index(drop=True)\n",
    "\n",
    "# Step 1: Create a DataFrame with 7200 rows and 3 columns, all filled with 0.0\n",
    "buffered_df_dolphins = pd.DataFrame(0.0, index=range(7200), columns=filtered_df_dolphins.columns)\n",
    "\n",
    "# Step 2: Copy the data from `filtered_df_dolphins` starting at row 2541\n",
    "buffered_df_dolphins.iloc[2541:2541+len(filtered_df_dolphins)] = filtered_df_dolphins.values\n",
    "\n",
    "# The final DataFrame `buffered_df_dolphins` will have the required structure\n",
    "#buffered_df_dolphins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f95255b-b85b-4009-9c18-a766cffb6215",
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = new_df_temp['depth']  # normalize\n",
    "col2 = new_df_temp['Pressure']  # normalize\n",
    "col3 = new_df_temp['temperature']  # normalize\n",
    "col4 = new_df_salinity['salinity'] # normalize\n",
    "col5 = new_df_U['U'] \n",
    "col6 = new_df_V['current']\n",
    "col7 = new_df_W['W']\n",
    "\n",
    "col8 = df_buoy['WVHT'] \n",
    "col9 = df_buoy['ATMP'] \n",
    "col10 = df_buoy['WSPD'] \n",
    "col11 = df_seismic['Sample'] #note space\n",
    "col12 = df_buoy['sun_exposure']*df_buoy['WSPD']\n",
    "\n",
    "col13 = df_rain['rain']\n",
    "# Select the columns from df_clouds\n",
    "col14 = df_clouds[[\"N\", \"NE\", \"E\", \"SE\", \"S\", \"SW\", \"W\", \"NW\"]]\n",
    "\n",
    "col15 = buffered_df_dolphins[[\"Lizzy_angle\",\"Perry_angle\",\"BentHighNick_angle\"]]\n",
    "\n",
    "merged_df = pd.DataFrame({'depth': col1, 'pressure': col2, 'temperature': col3, 'glitter':col12})\n",
    "normalized_df = merged_df.apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "#add ECCO Model, already normalized: \n",
    "normalized_df['salinity'] = col4\n",
    "normalized_df['U_current'] = col5\n",
    "normalized_df['V_current'] = col6 \n",
    "normalized_df['W_current'] = col7\n",
    "\n",
    "#add buoy, already normalized\n",
    "normalized_df[\"buoy_height\"] = col8\n",
    "normalized_df[\"buoy_temp\"] = col9 \n",
    "normalized_df[\"buoy_windspeed\"] = col10\n",
    "#add Seismic, already normalized\n",
    "normalized_df[\"seismic\"] = col11\n",
    "#add Rain, already normalized\n",
    "normalized_df[\"rain\"] = col13\n",
    "#add Clouds, already normalized\n",
    "# Assign them to normalized_df\n",
    "normalized_df[[\"N\", \"NE\", \"E\", \"SE\", \"S\", \"SW\", \"W\", \"NW\"]] = col14\n",
    "\n",
    "normalized_df[[\"Lizzy\",\"Perry\",\"BentHighNick\"]] = col15\n",
    "\n",
    "normalized_df.fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37f2125-749b-4645-8de9-93ca3c7ac703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_buoy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafe9f37-ed80-49ff-a491-0fd770194e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "# Define the number of unique colors equal to the number of columns\n",
    "colors = cm.get_cmap('tab20', len(normalized_df.columns))  # Using 'tab10' colormap (or any other colormap)\n",
    "\n",
    "# Create a single plot for all columns\n",
    "fig, ax1 = plt.subplots(figsize=(16, 4))\n",
    "\n",
    "# Loop through each column and plot on the same graph\n",
    "for idx, column in enumerate(normalized_df.columns):\n",
    "    ax1.plot(normalized_df.index, normalized_df[column], color=colors(idx), label=column)\n",
    "\n",
    "# Set labels for the x and y axes\n",
    "ax1.set_xlabel('Index')\n",
    "ax1.set_ylabel('Values')\n",
    "\n",
    "# Set a title for the plot\n",
    "plt.title('All Channels on the Same Graph')\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(output_path_figures+\"dive_nrt.png\")\n",
    "\n",
    "# Display the plot\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc970132-ca22-4ffa-8ef4-d81689f4950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "# Plot the 'glitter' column from normalized_df\n",
    "plt.plot(normalized_df.index, normalized_df[\"glitter\"], color=colors(idx), label=\"Glitter\")\n",
    "\n",
    "# Set labels for the x and y axes\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Values')\n",
    "\n",
    "# Set a title for the plot\n",
    "plt.title('Glitter Only')\n",
    "\n",
    "# Add a legend to the plot\n",
    "plt.legend()\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure (uncomment the line below to save the image)\n",
    "# plt.savefig(output_path_figures+\"dive.png\")\n",
    "\n",
    "# Display the plot\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11415410-032d-4bbf-a3ed-e6021b673243",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "# Plot the 'glitter' column from normalized_df\n",
    "plt.plot(normalized_df.index, normalized_df[\"salinity\"], color=colors(idx), label=\"Salinity\")\n",
    "\n",
    "# Set labels for the x and y axes\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Values')\n",
    "\n",
    "# Set a title for the plot\n",
    "plt.title('Glitter Only')\n",
    "\n",
    "# Add a legend to the plot\n",
    "plt.legend()\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure (uncomment the line below to save the image)\n",
    "# plt.savefig(output_path_figures+\"dive.png\")\n",
    "\n",
    "# Display the plot\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e449de-6cb8-4b63-a1a6-9b26283e9123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalized_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4f79d0-9a71-4b22-977f-06c1d1cac3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = output_path_nrt + '12min_pass_v08.csv' \n",
    "normalized_df.to_csv(file_name, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c7503-d23e-4f03-a9e9-c8fea9d59fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Specify your bucket name and file\n",
    "bucket_name = 'heart-ocean-public'\n",
    "file_name_out = 'nrt-data/12min_pass_v08.csv' \n",
    "\n",
    "# Upload the file\n",
    "s3.upload_file(file_name, bucket_name, file_name_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947c7f7f-ed95-4610-811a-3a2d59460c51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "user-env:(heart)",
   "language": "python",
   "name": "heart"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
