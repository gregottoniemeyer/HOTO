{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18dfcb9-c660-4071-9672-9c3a90745ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary libraries\n",
    "import pandas as pd\n",
    "#data comes from: \n",
    "#http://service.iris.edu/fdsnws/dataselect/1/query?net=CU&sta=GRTK&loc=00&cha=BHZ&starttime=2024-09-06T00:00:00&endtime=2024-09-06T00:01:00&format=csv\n",
    "\n",
    "from obspy import read\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "\n",
    "#import configuration location and filepath\n",
    "from myconfig import *\n",
    "\n",
    "\n",
    "# Initialize date_now and date_proc\n",
    "date_now = datetime.utcnow()  # Using UTC\n",
    "date_start = date_now - timedelta(minutes=1)\n",
    "\n",
    "# Convert to string formats for URL\n",
    "date_str1 = date_start.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "date_str2 = date_now.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "# Define initial URL with formatted date range\n",
    "url_template = 'http://service.iris.edu/fdsnws/dataselect/1/query?net=CU&sta=GRTK&loc=00&cha=BHZ&starttime={}&endtime={}&format=miniseed'\n",
    "success = False\n",
    "\n",
    "# Increment endtime until a successful response\n",
    "while not success:\n",
    "    url = url_template.format(date_str1, date_str2)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 204:\n",
    "        print(\"No content available. Incrementing endtime by 1 minute...\")\n",
    "        date_start -= timedelta(minutes=1)\n",
    "        date_str1 = date_start.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    else:\n",
    "        try:\n",
    "            response.raise_for_status()  # Raises an error if other non-2xx status codes are encountered\n",
    "            success = True  # Exit the loop if successful\n",
    "            print(\"Data successfully retrieved.\")\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"Request failed: {e}. Retrying with an incremented endtime...\")\n",
    "            date_now += timedelta(minutes=1)\n",
    "            date_str2 = date_now.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "            \n",
    "\n",
    "print(f\"Successful URL: {url}\")\n",
    "\n",
    "# Read the MiniSEED data into an ObsPy Stream object\n",
    "stream = read(BytesIO(response.content))\n",
    "\n",
    "# Convert the Stream into a Trace object (there may be multiple Traces in a Stream)\n",
    "trace = stream[0]\n",
    "\n",
    "# Create a DataFrame from the Trace data\n",
    "# Extract the time vector and the waveform data (trace data)\n",
    "time = trace.times(\"timestamp\")  # get timestamps\n",
    "data = trace.data  # get the waveform data\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame({'Time': time, 'Sample': data})\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98008ef-ad40-4bea-8f4a-dedaa688e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "#file_path = '../data/CU.GRTK.00.BHZ.M.2024-09-06T000000.019539.csv'\n",
    "#df = pd.read_csv(file_path, comment='#')\n",
    "\n",
    "#Time, Sample 2024-09-06T00:00:00.019539Z, 7046\n",
    "\n",
    "# Convert DATE and TIME to a single datetime column and round up to the next hour\n",
    "#date = datetime.fromtimestamp(timestamp)\n",
    "df['DATETIME'] = df['Time'].apply(lambda ts: datetime.fromtimestamp(ts)) #datetime.fromtimestamp(df['Time'])\n",
    "#df['DATETIME'] = df['DATETIME'].dt.ceil('h')  # Round to the next hour\n",
    "\n",
    "# Set DATETIME as the index\n",
    "df.set_index('DATETIME', inplace=True)\n",
    "\n",
    "#df_data = df[[' Sample']]\n",
    "df_data = df[['Sample']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dabbe6-9e8a-42f5-bafc-586757ca8d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fce3eb-11f2-4945-adc5-100ab5e4ebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = len(df_data)\n",
    "#print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f5dcb-48b0-46ad-90ff-d57db1b3ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb46567c-d10e-494d-adb3-7d01737703ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming df_data has 9600 rows\n",
    "rows = len(df_data)  # Total number of rows in the DataFrame (9600)\n",
    "target_rows = 1800  # Desired number of rows after downsampling\n",
    "\n",
    "# Calculate the number of rows per group (group size)\n",
    "group_size = rows / target_rows  # This results in 5.3333...\n",
    "\n",
    "# Create groups dynamically using np.floor(np.arange())\n",
    "df_data['group'] = np.floor(np.arange(rows) / group_size).astype(int)\n",
    "\n",
    "# Now downsample by taking the mean (or max) of each group\n",
    "downsampled_df = df_data.groupby('group').max()  # Or use .max(), depending on your requirement\n",
    "\n",
    "# Drop the 'group' column if necessary\n",
    "# downsampled_df.drop(columns='group', inplace=True)\n",
    "\n",
    "# Display the first few rows of the downsampled DataFrame\n",
    "#downsampled_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e8f3de-dc27-4b89-9efa-d61c523912d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downsampled_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85344937-1958-49bd-b3c8-5eb2f7a1e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(downsampled_df.index,downsampled_df[\"Sample\"])\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.savefig(output_path_figures+\"seismic_realtime.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f237942e-6ec2-4801-b4c4-5f033cdc7578",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_df.to_csv(output_path_seismic+\"seismic_1800.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692506fb-6963-44a3-9fff-09b21b981579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize and park at 3600\n",
    "\n",
    "\n",
    "# Assuming 'df' is already loaded and has the 'Sample' column\n",
    "# Normalize the 'Sample' column using min-max normalization\n",
    "normalized_sample = (downsampled_df['Sample'] - downsampled_df['Sample'].min()) / (downsampled_df['Sample'].max() - downsampled_df['Sample'].min())\n",
    "\n",
    "# Create a new DataFrame with an index from 0 to 7199\n",
    "new_df = pd.DataFrame(index=np.arange(7200))\n",
    "\n",
    "# Fill the new DataFrame with 0.0 values initially\n",
    "new_df['Sample'] = 0.0\n",
    "\n",
    "# Find the start index to center the data at index 3599\n",
    "start_idx = 3599\n",
    "\n",
    "# Place the normalized data in the new DataFrame starting at 'start_idx'\n",
    "new_df.loc[start_idx:start_idx + len(normalized_sample) - 1, 'Sample'] = normalized_sample.values\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "#new_df.head(10), new_df.tail(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc28390a-a604-4858-a500-6935de9907ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccb9376-1457-45bb-9d29-01fff4fe5984",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(new_df.index,new_df[\"Sample\"])\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.savefig(output_path_figures+\"seismic_realtime_7200.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff566d-02a2-4df0-9f86-4804a146ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = output_path_nrt + 'seismic_7200.csv'\n",
    "new_df.to_csv(fout, index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea95f7-3780-4857-b5be-ddbf4c61456b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef28e5eb-2027-4f61-a755-c92727c3b32a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "user-env:(heart)",
   "language": "python",
   "name": "heart"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
